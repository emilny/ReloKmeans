{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ReloK-means.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emilny/ReloKmeans/blob/main/ReloK_means.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXhI4MlycG_X"
      },
      "source": [
        "# 0. Initial package installment\n",
        "Before imports can be made, some packages need to be installed.\n",
        "\n",
        "Using `pip`, the following commands must be run in the terminal, or through the % magic command in a notebook environment:\n",
        "\n",
        "```\n",
        "pip install numpy\n",
        "pip install scipy\n",
        "pip install pandas\n",
        "pip install matplotlib\n",
        "pip install sklearn\n",
        "pip install progressbar\n",
        "pip install joblib\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXDU1znrv8PV"
      },
      "source": [
        "# All necessary packages and functions imports here\n",
        "import random\n",
        "import numpy as np\n",
        "from scipy.spatial import distance_matrix\n",
        "import pandas as pd\n",
        "%matplotlib\n",
        "%matplotlib notebook\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from sklearn.cluster import KMeans\n",
        "from google.colab import drive\n",
        "import progressbar\n",
        "from joblib import Parallel, delayed\n",
        "import joblib\n",
        "import math\n",
        "import copy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcX9m-vJqoaU"
      },
      "source": [
        "# 1. Data\n",
        "This snippet contains a function which reads data from `filename` and returns it in a pandas dataframe format. It also finds the file containing ground truth centroids, which is similarly returned as a dataframe (and a numpy array for certain special use cases) together with an inferred number of clusters `k`.\n",
        "\n",
        "Be sure to change the path before running, as this will depend on data location."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STb72htLT_6S"
      },
      "source": [
        "#drive.mount('/content/gdrive/') # When using google drive to store data sets\n",
        "\n",
        "\n",
        "def read_data_and_gt(filename):\n",
        "  \"\"\"\n",
        "  Function to read from drive based on prefix of filename (i.e. \"s1\")\n",
        "  :param filename: prefix of filename\n",
        "  :return: Data in df, ground truth in df, and gt in ndarray, number of clusters k\n",
        "  \"\"\"\n",
        "  # path = \"/content/gdrive/My Drive/Maskinlæring/Prosjekt/datasets\" # When using google drive to store data sets\n",
        "  path = \"/data\" # If data sets are in the same folder as code\n",
        "\n",
        "  if filename[:3] == \"dim\":\n",
        "    dims = int(filename[3:])\n",
        "    # Reads in higher dimensional data set and ground truth as pandas dataframe\n",
        "    df = pd.read_csv(f\"{path}/{filename}.txt\", delim_whitespace=\"    \", names = [dim for dim in range(dims)])\n",
        "    ground_truth_df = pd.read_csv(f\"{path}/{filename}_gt.txt\", delim_whitespace=\"    \",  names = [dim for dim in range(dims)])\n",
        "  else:\n",
        "    # Reads in 2D data set and ground truth as pandas dataframe\n",
        "    df = pd.read_csv(f\"{path}/{filename}.txt\", delim_whitespace=\"    \", names = ['x', 'y'])\n",
        "    ground_truth_df = pd.read_csv(f\"{path}/{filename}_gt.txt\", delim_whitespace=\"    \", names = ['x', 'y'])\n",
        "\n",
        "  # Convert centroids to simple format [[point], [point],...]\n",
        "  gt = ground_truth_df.values.tolist()\n",
        "\n",
        "  k = len(gt) # Number of clusters in data set will depend on ground truth\n",
        "\n",
        "  return df, ground_truth_df, gt, k\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b23Suc90AJ5"
      },
      "source": [
        "# 2. Support functions\n",
        "The code blocks below contain functions that are used for intermediate calculations inside algorithms and during experiments, as well as for format conversions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMXQFdDLwdpz"
      },
      "source": [
        "def distance(a, b, p):\n",
        "    \"\"\"\n",
        "    Returns power distance between two points\n",
        "    (p = 1 gives manhattan, p = 2 gives squared euclidian and so on)\n",
        "    NOTE: This is not the same as minkowski distance, because the final number isn't raised to 1/p\n",
        "    :param a: input point a\n",
        "    :param b: input point b\n",
        "    :param p: power to calculate distance in\n",
        "    :return: distance between points raised to power p\n",
        "    \"\"\"\n",
        "    diff = np.array(a) - np.array(b)\n",
        "    diff = abs(diff)\n",
        "    return sum(diff**p)\n",
        "\n",
        "# Examples\n",
        "print(distance([0,0], [2,2], 1))\n",
        "print(distance([0,0], [2,2], 2))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLUdRBWec-_f"
      },
      "source": [
        "def SSE(cluster_pts):\n",
        "    \"\"\"\n",
        "    Calculates the Sum Squared Error (variance) within a cluster\n",
        "    :param cluster_pts: list of points in a single cluster\n",
        "    :return: float SSE\n",
        "    \"\"\"\n",
        "    center = tuple(np.average(cluster_pts, axis=0))\n",
        "    return sum(distance(center, point, 2) for point in cluster_pts)\n",
        "\n",
        "# Examples\n",
        "print(SSE([(1,1),(2,2),(3,3)]))\n",
        "print(SSE([(1,1),(1,1),(1,1)]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_wegbEbwlTB"
      },
      "source": [
        "def score(clustering):\n",
        "  \"\"\"\n",
        "  Finds score (total SSE) of an output clustering. That is, total SSE for all clusters in a partition\n",
        "  :param clustering: dictionary with {index : List of list of points (i.e. list of clusters)}\n",
        "  :return: float total SSE \n",
        "  \"\"\"\n",
        "  clusters = clustering.values()\n",
        "  return sum(SSE(cluster_pts=cluster) for cluster in clusters)\n",
        "\n",
        "# Example\n",
        "print(score({0: [(1,1),(2,2),(3,3)],\n",
        "             1: [(1,1),(1,1),(1,1)]}))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yve2Eb98EdBT"
      },
      "source": [
        "def CI(gt, centroids):\n",
        "  \"\"\"\n",
        "  Function for comparing an output clustering (represented by centroid positions) to ground truth\n",
        "  Centroid Index was introduced by:\n",
        "  https://www.sciencedirect.com/science/article/pii/S0031320314001150#bib21\n",
        "\n",
        "  Maps each centroid in gt to its closest neighbor in \"centroids\",\n",
        "  Returns number of centroids in \"centroids\" that have no gt centroids mapped to it \n",
        "  (Such centroids are called orphans)\n",
        "  \n",
        "  :param gt: Ground truth centroids in ndarray\n",
        "  :param centroids: Result centroids after running Kmeans or ReloKmeans\n",
        "  :return: CI-value for a clustering\n",
        "  \"\"\"\n",
        "  assert len(gt) == len(centroids)\n",
        "\n",
        "  dist_matrix = distance_matrix(gt, centroids)**2 # Squared euclidian distances\n",
        "\n",
        "  gt_to_centroids_mapping = [] # [2, 0,...] means gt[0]->centroids[2], gt[1]->centroids[0], ...\n",
        "  for row in dist_matrix:\n",
        "    gt_to_centroids_mapping.append(np.argmin(row))\n",
        "\n",
        "  # Orphans are not mapped to by any gt centroid, want to return number of orphans\n",
        "  orphans = set([i for i in range(len(centroids)-1)])-set(gt_to_centroids_mapping)\n",
        "  \n",
        "  return len(orphans)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qpsd1V6qc15"
      },
      "source": [
        "def convert_to_dict(data, labels):\n",
        "    \"\"\"\n",
        "    Function for converting KMeans output labels and data set into dictionary of labeled clusters\n",
        "    Easier to work with, but computationally more exepensive. Could be optimized to decrease total run time.\n",
        "    :param data: Data points in pandas dataframe format\n",
        "    :param labels: labels for each point (row) in dataframe\n",
        "    :return: Labeled clusters. Dict with key:values as label : List<points>\n",
        "    \"\"\"\n",
        "    matrix = data.values.tolist()\n",
        "    clustering = {}\n",
        "    for i, label in enumerate(labels):\n",
        "        if label in clustering.keys():\n",
        "            clustering[label].append(tuple(matrix[i]))\n",
        "        else:\n",
        "            clustering[label] = [tuple(matrix[i])]\n",
        "    return clustering"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqH8aKpflgxd"
      },
      "source": [
        "def convert_to_pd_series(centroids):\n",
        "    \"\"\"\n",
        "    Function to convert data for 2D-plotting\n",
        "    :param centroids: list of centroids\n",
        "    :return: lists of x coordinates and y coordinates of centroids\n",
        "    \"\"\"\n",
        "    centroids_x = pd.Series([point[0] for point in centroids])\n",
        "    centroids_y = pd.Series([point[1] for point in centroids])\n",
        "    return centroids_x, centroids_y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jV-GHdQ2-DB"
      },
      "source": [
        "# 3. Components of the ReloKmeans algorithm\n",
        "`reduce_centroids()` carries out the step of merging closest pair of centroids within the `relocate()` algorithm, i.e. \"freeing up\" centroids that can then be distributed elsewhere. The function is generalized to accomodate `n>1` as well, meaning relocating multiple centroids in a single step of ReloKmeans (discussed in Section 7 \"Conclusion\" for further work in project paper). \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4lAfVvDwoVR"
      },
      "source": [
        "def reduce_centroids(centroids, n):\n",
        "    \"\"\"\n",
        "    Recursively reduces list of centroids by joining closest pair until new list of centroids contains n less than input\n",
        "    :param centroids: Input centroid to be reduced\n",
        "    :param n: number of centroids to remove\n",
        "    :return: List of new centroids \n",
        "    \"\"\"\n",
        "    k = len(centroids)\n",
        "\n",
        "    # Base case\n",
        "    if n == 0:\n",
        "        return centroids\n",
        "\n",
        "    centroid_distances = distance_matrix(centroids, centroids)\n",
        "    # Checking half of distance matrix excluding diagonal\n",
        "    centroid_pairs = []\n",
        "    for i in range(1, k):\n",
        "        for j in range(0, i):\n",
        "            centroid_pairs.append((i, j))\n",
        "    sorted_pairs_by_dist = sorted(centroid_pairs, key=lambda pair: centroid_distances[pair[0], pair[1]])\n",
        "    closest_pair = sorted_pairs_by_dist.pop(0)\n",
        "    index1, index2 = closest_pair[0], closest_pair[1]\n",
        "    c1, c2 = centroids[index1], centroids[index2]\n",
        "    joined_centroid = tuple(np.average([c1, c2], axis=0))\n",
        "    centroids.remove(c1)\n",
        "    centroids.remove(c2)\n",
        "    centroids.append(joined_centroid)\n",
        "    return reduce_centroids(centroids, n-1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VX32bDVQ4AA2"
      },
      "source": [
        "`relocate()` is used for the relocation step that is carried out between calls to KMeans within ReloKmeans. This corresponds to Algorithm 1 which is described in Section 3.2 \"Pseudocode\" in the project paper.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUbKoP27xDzj"
      },
      "source": [
        "def relocate(clusters, n=1):\n",
        "    \"\"\"\n",
        "    Relocates centroids from dense areas to high-variance clusters.\n",
        "    Step one: Finding n \"worst\" clusters, i.e. clusters with highest variance\n",
        "    Step two: Finding n closest pairs of centroids (excluding worst), merging these into one, thus freeing up n centroids\n",
        "    Step three: Placing two centroids instead of one in each of the n worst clusters, using a furthest point heuristic\n",
        "    :param clusters: Dictionary containing numbered clusters (Output after running Kmeans on a data set)\n",
        "    :param n: number of clusters to improve at once. Default is 1 because multiple at once might cause optimal solution \n",
        "    to be skipped\n",
        "    :return: list of new centroid positions, i.e. input for another initialization of Kmeans.\n",
        "    \"\"\"\n",
        "    k = len(clusters.keys())\n",
        "    # Verify that n is not larger than k/2, as that would not make sense for this operation\n",
        "    assert n < k / 2\n",
        "    \n",
        "    # Step one\n",
        "    n_worst_clusters = sorted(clusters.values(), key=lambda cluster_pts: SSE(cluster_pts), reverse=True)[:n]\n",
        "    n_worst_centroids = [tuple(np.average(cluster_pts, axis=0)) for cluster_pts in n_worst_clusters]\n",
        "\n",
        "    # Step two\n",
        "    remaining_clusters = sorted(clusters.values(), key=lambda cluster_pts: SSE(cluster_pts), reverse=True)[n:]\n",
        "    remaining_centroids = [tuple(np.average(cluster, axis=0)) for cluster in remaining_clusters]\n",
        "    new_centroids = reduce_centroids(remaining_centroids, n)\n",
        "\n",
        "    # Verify that n centroids have been freed up by joining n close pairs (n worst are also currently missing at this point)\n",
        "    assert len(new_centroids) == k-2*n  \n",
        "\n",
        "    # Step three \n",
        "    # (ignores old centroid, places two new by first random selection and then point furthest away using manhattan dist)\n",
        "    for cluster in n_worst_clusters:\n",
        "        first_centroid = [random.choice(cluster)].copy()[0]\n",
        "        furthest_away_centroid = sorted(cluster, key=lambda point: distance(point, first_centroid, 1))[-1]\n",
        "        new_centroids.extend([first_centroid, furthest_away_centroid])\n",
        "    \n",
        "    # Verify that final number of centroids has not changed\n",
        "    assert len(new_centroids) == k\n",
        "\n",
        "    return new_centroids\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rf_aOsip5PKU"
      },
      "source": [
        "# 4. The ReloKmeans algorithm\n",
        "The code block below contains the function for running ReloKmeans on a data set. This algorithm is the nucleus of the project paper.\n",
        "\n",
        "The implementation is generalized to accomodate `n>1` as well, meaning relocating multiple centroids between runs of `KMeans()`. To ensure that the best solution is returned when using `n>1`, the algorithm is re-run with `n=1` (skipping the initialization step) with the current best clustering once the call with `n>1` terminates. To understand why this has to be done, imagine the initial clustering having CI=9, i.e. 9 misplaced centroids. If `n=4`, three relocation steps would most likely be executed, the last step \"overshooting\" by 3 (CI would most likely take on values 9, 5, 1, 3), and the returned result would have a CI value of 1. Instead, the algorithm re-runs on the returned clustering with `n=1`, likely performing two relocation steps (CI taking on values 1, 0, 1) before returning the result with CI=0. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-0jsMqaZ3fR"
      },
      "source": [
        "def ReloKmeans(data, k=15, n=1, random_initial=True, rec_params=None):\n",
        "    \"\"\"\n",
        "    Repeats Kmeans while relocating potentially misplaced centroids between every repeat.\n",
        "    Repeats continue until result stops improving, i.e. when score(clustering) increases.\n",
        "    When score of a clustering increases compared to last, the previous clustering is returned\n",
        "    \n",
        "    Uses Sklearn's KMeans to perform clustering in each run.\n",
        "\n",
        "    :param rec_params: Parameters passed in case n>1 and a recursive call is made with n=1.\n",
        "                        format will be (result_clustering, scores, plotting_list)\n",
        "    :param random_initial: Whether to use random or Kmeans++ initialization for first run\n",
        "    :param data: input data points in pandas dataframe format\n",
        "    :param k: number of clusters\n",
        "    :param n: number of centroids to relocate at a time. Default is 1 to avoid \"skipping over\" best solution\n",
        "              If n>1 recursive call is made with n=1 to ensure best solution is found\n",
        "    :return: clustering solution;\n",
        "              plotting_list (list of the different configurations of centroids that have occurred throughout the algorithm);\n",
        "              scores (list of the different scores that have been acheived throughout the algorithm, for plotting purposes)\n",
        "              run_times (containing run times of each call to KMeans, for plotting purposes)\n",
        "    \"\"\"\n",
        "    run_times = [] # Recording run times for each call to KMeans()\n",
        "    # Checking if we are in recursive call or not\n",
        "    if rec_params is None:\n",
        "        \n",
        "        start = time.time()\n",
        "        \n",
        "        km = KMeans(n_clusters=k,\n",
        "                    init=\"random\" if random_initial else \"k-means++\",\n",
        "                    n_init=1,  # Don't want to run Kmeans multiple times with our algorithm (default is 10 repeats)\n",
        "                    ).fit(data.to_numpy())\n",
        "        run_times.append(time.time()-start)\n",
        "\n",
        "        # Output of initial run is scored\n",
        "        current_clustering = convert_to_dict(data, km.labels_)\n",
        "        current_score = score(current_clustering)\n",
        "\n",
        "        # Keeping track of centroids produced for plotting purposes\n",
        "        plotting_list = []\n",
        "        current_centroids = km.cluster_centers_\n",
        "        plotting_list.append(current_centroids)\n",
        "\n",
        "        # Keeping track of scores (SSE) for different runs\n",
        "        scores = [current_score]\n",
        "    else:\n",
        "        current_clustering = rec_params[0]\n",
        "        scores = rec_params[1]\n",
        "        plotting_list = rec_params[2]\n",
        "        run_times = rec_params[3]\n",
        "        current_score = min(scores) # Best score reached from previous call\n",
        "\n",
        "    # Try to improve by relocating n centroids\n",
        "    new_centroids = relocate(current_clustering, n=n)\n",
        "\n",
        "    start = time.time()\n",
        "    # Run K-means again with new initial centroids (improved centroids from previous run)\n",
        "    new_km = KMeans(n_clusters=k,\n",
        "                    init=np.array(new_centroids),\n",
        "                    n_init=1,\n",
        "                    # Don't want to run Kmeans multiple times with our algorithm (default is 10 repeats)\n",
        "                    ).fit(data.to_numpy())\n",
        "    run_times.append(time.time()-start)\n",
        "    \n",
        "    # Output of new run is scored\n",
        "    new_clustering = convert_to_dict(data, new_km.labels_)\n",
        "    new_score = score(new_clustering)\n",
        "\n",
        "    # Score is saved\n",
        "    scores.append(new_score)\n",
        "\n",
        "    while new_score < current_score:  # Continue improving centroids until score stops decreasing\n",
        "\n",
        "        # Record new centroids for plotting (Don't want to plot last centroids, since these will be worse\n",
        "        result_centroids = new_km.cluster_centers_\n",
        "        plotting_list.append(result_centroids)\n",
        "\n",
        "        # Move \"current\" to new, since new_score was better\n",
        "        current_score = new_score\n",
        "        current_clustering = new_clustering\n",
        "\n",
        "        # Try to improve by relocating n centroids\n",
        "        new_centroids = relocate(current_clustering, n=n)\n",
        "\n",
        "        start = time.time()\n",
        "        # Run K-means again with new initial centroids\n",
        "        new_km = KMeans(n_clusters=k,\n",
        "                        init=np.array(new_centroids),\n",
        "                        n_init=1 # Don't want to run Kmeans multiple times with our algorithm (default is 10 repeats)\n",
        "                        ).fit(data.to_numpy())\n",
        "        run_times.append(time.time()-start)\n",
        "\n",
        "        # Output of new run is scored\n",
        "        new_clustering = convert_to_dict(data, new_km.labels_)\n",
        "        new_score = score(new_clustering)\n",
        "\n",
        "        # Score is saved\n",
        "        scores.append(new_score)\n",
        "\n",
        "    # If n was >1 then we need to check result with n=1 to make sure optimal solution was not overshot\n",
        "    if n > 1:\n",
        "        rec_params = (current_clustering, scores, plotting_list)\n",
        "        return ReloKmeans(data, k=k, n=1, max_iter=max_iter, rec_params=rec_params)\n",
        "\n",
        "    return current_clustering, plotting_list, scores, run_times"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_irz0jcDFXm"
      },
      "source": [
        "# 5. Experiments\n",
        "Code blocks below are used for performing trials and experiments on the algorithms. \n",
        "\n",
        "As described in Section 5 \"Experiments\" in the project paper, Repeated Kmeans is tested somewhat differently compared to ReloKmeans.\n",
        "`repeat_until_correct()` measures the required attempts to acheive CI=0 for Repeated Kmeans, and returns the lowest CI-value in trials where CI=0 isn't acheived before `attempts=cutoff`.\n",
        "\n",
        "ReloKmeans is simply called once on the data set in a single trial, and carries out calls to `KMeans()` on its own until termination. The number of attempts is then easily inferred from the number of scores recorded within ReloKmeans. CI-value is then recorded for that trial by checking the returned clustering.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KynO4OtR30P"
      },
      "source": [
        "def repeat_until_correct(data, gt, cutoff, random_initial, k):\n",
        "  \"\"\"\n",
        "  Function for repeating KMeans until a result is correct (CI=0). \n",
        "  USED FOR RUNNING A SINGLE TRIAL WITHIN multiple_trials()\n",
        "  If CI=0 is not reached after cutoff repeats the function returns attempts = cutoff \n",
        "  and lowest CI-value acheived before cutoff\n",
        "  :params: See multiple_trials() description\n",
        "  :return: (attempts, min_ci(=0)) or (attempts(=cutoff), min_ci)\n",
        "  \"\"\"\n",
        "  km = KMeans(n_clusters=k,\n",
        "              init=\"random\" if random_initial else \"k-means++\",\n",
        "              n_init=1,  # Don't want to run Kmeans multiple times with our algorithm (default is 10 repeats)\n",
        "              ).fit(data.to_numpy())\n",
        "  result = km.cluster_centers_\n",
        "  ci = CI(gt, result)\n",
        "  min_ci = ci # Keeping track of lowest ci acheived in case no success until cutoff\n",
        "  attempts = 1\n",
        "  while attempts < cutoff and ci!=0:\n",
        "    km = KMeans(n_clusters=k,\n",
        "              init=\"random\" if random_initial else \"k-means++\",\n",
        "              n_init=1,  # Don't want to run Kmeans multiple times with our algorithm (default is 10 repeats)\n",
        "              ).fit(data.to_numpy())\n",
        "    result = km.cluster_centers_\n",
        "    ci = CI(gt, result)\n",
        "    min_ci = min((min_ci, ci))\n",
        "    attempts += 1\n",
        "  return attempts, min_ci\n",
        "\n",
        "def mulitple_trials(n_trials, data, gt, k, n, relokm=True, random_initial=True, cutoff=10):\n",
        "  \"\"\"\n",
        "  Function for running ReloKmeans(&++) or KMeans(&++) multiple times and recording number of calls to\n",
        "  Kmeans until convergence for each execution, as well as CI index when termination does not occur before cutoff\n",
        "  :param n_trials: number of trials to record\n",
        "  :param data: pandas dataframe containing data points to be clustered\n",
        "  :param gt: ground truth centroids for data set (as ndarray)\n",
        "  :param k: number of clusters\n",
        "  :param n: number of centroids to relocate at a time, default is 1\n",
        "  :param relokm: Boolean value deciding which of the clustering algorithms to use\n",
        "  :param random_initial: Boolean value deciding whether to use naive or k++ initialization\n",
        "  :param cutoff: Number of attempts to allow for in each trial of Repeated KMeans\n",
        "  :return: list of CI values acheived, list of number of calls to KMeans. \n",
        "  Results from first trial correspond to first value in CI_values and first number in number_of_calls (and so on)\n",
        "  \"\"\"\n",
        "  number_of_calls = []\n",
        "  CI_values = []\n",
        "\n",
        "  # Progress bar for monitoring process \n",
        "  bar = progressbar.ProgressBar(maxval=n_trials, \\\n",
        "      widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
        "  bar.start()\n",
        "  \n",
        "  if relokm:\n",
        "    for i in range(n_trials): \n",
        "      clustering, plotting_list, scores, run_times = ReloKmeans(data, random_initial=random_initial, k=k, n=n)\n",
        "      result = plotting_list[-1]\n",
        "      ci = CI(gt, result)\n",
        "      number_of_calls.append(len(scores)) # Records scores after every call, so just count those\n",
        "      CI_values.append(ci)\n",
        "      bar.update(i+1)\n",
        "      #if ci >0:\n",
        "      #  gt_df = pd.DataFrame(np.array(gt), columns=[\"x\",\"y\"])\n",
        "      #  plot_centroids([plotting_list[0], plotting_list[-1]], gt_df=gt_df)\n",
        "  else:\n",
        "    for i in range(n_trials): \n",
        "      attempts, min_ci = repeat_until_correct(data, gt, cutoff, random_initial, k)\n",
        "      number_of_calls.append(attempts)\n",
        "      CI_values.append(min_ci) # Whenever min_ci is not 0 we know cutoff happened\n",
        "      bar.update(i+1)\n",
        "\n",
        "  bar.finish()\n",
        "  return CI_values, number_of_calls\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUc89wQ6Dxwy"
      },
      "source": [
        "Multiprocessing was used to carry out tasks that could be performed independently - this saves a substantial amount of time when multiple CPUs are available. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hsp1N7eAxTq"
      },
      "source": [
        "def collect_tasks_for_each_algo(filename, n_trials = 1000, cutoff=100):\n",
        "  \"\"\"\n",
        "  Collects tasks for processes to run n_trials on data set and record results for all 4 algorithms\n",
        "  :param filename: name of data set, e.g. \"s1\"\n",
        "  :param n_trials: Number of trials to complete for each algo\n",
        "  :return: 4 tasks, one for each algo (delayed, using joblib)\n",
        "  \"\"\"\n",
        "  \n",
        "  df, ground_truth_df, gt, k = read_data_and_gt(filename)\n",
        "\n",
        "  # Arguments to pass into multiple_trials() to vary algo\n",
        "  algo_settings = {\"NKM\": (False, True),\n",
        "                  \"ReloKM\": (True, True),\n",
        "                  \"KM++\": (False, False),\n",
        "                  \"ReloKM++\": (True, False)\n",
        "                  }\n",
        "\n",
        "  # Collect 4 tasks: running n_trials on each algo\n",
        "  tasks = [delayed(mulitple_trials)(n_trials, df, gt, k, 1,algo_settings[alg][0], algo_settings[alg][1], cutoff) for alg in algo_settings.keys()]\n",
        "\n",
        "  return tasks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrZCF8KMIXfp"
      },
      "source": [
        "Code below runs trials by performing the collected tasks for each data set\n",
        "Number of tasks is then 4*(number of data sets), which can be done concurrently.\n",
        "When running, each process bar corresponds to a single algo performing `n_trials` on a single data set\n",
        "\n",
        "To perform trials: Provide list of data set names in `setnames` argument, and enter desired `n_trials` and `cutoff` into function call at the bottom."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVuqigb12zeM"
      },
      "source": [
        "def run_trials_on_all_data_sets(setnames, n_trials=1000, cutoff=100):\n",
        "  \"\"\"\n",
        "  Runs trials on a list of data sets\n",
        "  Runs on multiple processes if several CPUs are available\n",
        "  :param setnames: names of data sets\n",
        "  :param n_trials: Number of trials to perform for each algo on each data set\n",
        "  :param cutoff: See description in multuple_trials()\n",
        "  :return: Dictionary containing key:value as setname : algo_results, where \n",
        "  algo_results is again a dictionary with key:value as algo : result\n",
        "  \"\"\"\n",
        "  algo_names = [\"NKM\",\"ReloKM\", \"KM++\", \"ReloKM++\"]\n",
        "  data_set_results = {}\n",
        "\n",
        "  executor = Parallel(n_jobs=-1, backend= 'multiprocessing')\n",
        "  tasks = []\n",
        "  for setname in setnames:\n",
        "    tasks.extend(collect_tasks_for_each_algo(setname, n_trials, cutoff))\n",
        "  results = executor(tasks)\n",
        "  algo_results = {} # Temporarily holds results for each algo on a single data set\n",
        "\n",
        "  for i, algo_result in enumerate(results):\n",
        "    #Fill up result dict for set floor(i/4)\n",
        "    algo = algo_names[i%4]\n",
        "    algo_results[algo] = algo_result\n",
        "\n",
        "    if (i+1)%4 == 0: # Next idx will be for next set, so copy dict and add to results\n",
        "      setname = setnames[math.floor(i/4)]\n",
        "      data_set_results[setname] = copy.deepcopy(algo_results)\n",
        "  return data_set_results\n",
        "\n",
        "\n",
        "# DEFINE DATA SETS HERE\n",
        "# setnames = [\"s1\",\"s2\",\"s3\",\"s4\"]\n",
        "setnames = [\"a1\",\"a2\", \"a3\"]\n",
        "#setnames = [\"dim256\"]\n",
        "#setnames = [\"b2_k15\",\"b2_k25\",\"b2_k50\",\"b2_k75\", \"b2_k100\"]\n",
        "\n",
        "# ENTER DESIRED NUMBER OF TRIALS AND CUTOFF HERE\n",
        "# n_trials=10 for simplicity, increase this for more reliable results\n",
        "data_set_results = run_trials_on_all_data_sets(setnames, n_trials = 10, cutoff = 10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNX_sSwyKkTp"
      },
      "source": [
        "The code below calculates averages and relevant information from trial results, and prints them nicely. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajtsXMamRQNQ"
      },
      "source": [
        "def print_results(data_set_results):\n",
        "  \"\"\"\n",
        "  Function for printing results from trials of algorithms on data sets\n",
        "  :param data_set_results: Dictionary containing setname -> algo_results pairs\n",
        "  :return:\n",
        "  \"\"\"\n",
        "  for setname, algo_results in data_set_results.items():\n",
        "    #Prints out trial results nicely for every algo in algo results\n",
        "    print(\"=\"*100)\n",
        "    print(f\"{setname.upper()} Data Set\".rjust(27))\n",
        "    print(\"=\"*100)\n",
        "\n",
        "    for algo in algo_results.keys():\n",
        "      print(\"=\"*100)\n",
        "      print(f\"#################   {algo}    ######################################\")\n",
        "      ci_vals, ncalls = algo_results[algo] # Idx 0 means first trial, so ci_vals[0] is min ci acheived and ncalls[0] is number of calls made during first trial \n",
        "\n",
        "      success_ncalls = [] # All trials where ci_val=0\n",
        "      failed_ci_vals = [] # All trials where ci_val>0\n",
        "      failed_ncalls = [] # Number of calls before fail (will be equal to cutoff+1 for RKM)\n",
        "      for i, ci_val in enumerate(ci_vals):\n",
        "        if ci_val==0:\n",
        "          success_ncalls.append(ncalls[i])\n",
        "        else:\n",
        "          failed_ci_vals.append(ci_val)\n",
        "          failed_ncalls.append(ncalls[i])\n",
        "\n",
        "      n_success = len(success_ncalls)\n",
        "      n_fails = len(failed_ci_vals)\n",
        "\n",
        "      avg_success_ncalls = None if n_success == 0 else np.mean(success_ncalls)      # What is the average number of calls required to reach CI=0?\n",
        "      avg_fail_ci_val = None if n_fails == 0 else np.mean(failed_ci_vals)           # When it fails, what is the average CI value it acheives?\n",
        "      avg_fail_ncalls = None if n_fails == 0 else np.mean(failed_ncalls)            # When it fails, how many calls does it make on average?\n",
        "\n",
        "      print(\"Number of successes\".ljust(35), f\": {n_success}\")\n",
        "      print(\"Number of fails\".ljust(35), f\": {n_fails}\")\n",
        "      print(\"Average repetitions to reach CI=0\".ljust(35), f\": {avg_success_ncalls}\")\n",
        "      print(\"Average achieved CI at failure\".ljust(35), f\": {avg_fail_ci_val}\")\n",
        "      print(\"Average number of calls at failure\".ljust(35), f\": {avg_fail_ncalls}\")\n",
        "  \n",
        "\n",
        "print_results(data_set_results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ml2vck4LLR6B"
      },
      "source": [
        "# 6. Run time analysis\n",
        "Code below is used for experiments described in Section 5.2 \"Setup and Implementation\", and further discussed in Section 6.4 \"Run times within ReloKmeans\" in the project paper.\n",
        "\n",
        "Can be performed on different data sets by entering desired setname in function call at the bottom. `n_runs` defines how many runs to perform on the set. In the project paper, an average from `n_runs=1000` was deemed sufficient to illustrate a representative performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Whf5fpRA81RO"
      },
      "source": [
        "def find_avg_run_times(n_runs, setname):\n",
        "  \"\"\"\n",
        "  Function to run ReloKMeans a number of times and record average run time for each call to sklearn.KMeans() within the process\n",
        "\n",
        "  :param n_runs: number of runs to perform\n",
        "  :param setname: name of data set to perform trials on\n",
        "  :return: A list of average run times for 1st, 2nd, 3rd (and so on) calls to sklearn clustering function \n",
        "  \"\"\"\n",
        "  df, ground_truth_df, gt, k = read_data_and_gt(setname)\n",
        "\n",
        "  total = []\n",
        "  bar = progressbar.ProgressBar(maxval=n_runs, \\\n",
        "      widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
        "  bar.start()\n",
        "  for i in range(n_runs):\n",
        "    _, __, ___, run_times =  ReloKmeans(df,random_initial=False, k=k, n=1)\n",
        "    # Excluding last run time (which takes slightly longer) but we ignore this\n",
        "    # to illustrate the effect of closing in on global optimum\n",
        "    for j, runtime in enumerate(run_times): \n",
        "      bar.update(i+1)\n",
        "      if j>len(total)-1:\n",
        "        total.append(runtime)\n",
        "      else:\n",
        "        total[j] += runtime\n",
        "  bar.finish()\n",
        "  avg = np.array(total)/n_runs\n",
        "  return avg\n",
        "\n",
        "def plot_avg_run_times(avg):\n",
        "  \"\"\"\n",
        "  Function for plotting average run times\n",
        "  :param avg: List of average run times from function find_avg_run_times()\n",
        "  \"\"\"\n",
        "  plt.figure(figsize=[15,10])\n",
        "  plt.title(\"Average run time for each call to KMeans function\")\n",
        "  plt.rc('xtick', labelsize=20)\n",
        "  plt.rc('ytick', labelsize=20)\n",
        "  plt.xticks([i+1 for i in range(len(avg))], ('1st', '2nd', '3rd', '4th', '5th', '6th', '7th', '8th', '9th', '10th', '11th', '12th', '13th', '14th', '15th', '16th','17th','18th', '19th', '20th'))\n",
        "  plt.ylabel(\"Seconds\", fontsize=20)\n",
        "  plt.xlabel(\"Function calls\", fontsize=20)\n",
        "  plt.plot([i+1 for i in range(len(avg))], avg)\n",
        "\n",
        "\n",
        "# ENTER DESIRED SETNAME AND NUMBER OF RUNS HERE\n",
        "avg = find_avg_run_times(n_runs=10, setname=\"a3\") # n_runs=10 for simplicity, increase this for more reliable results\n",
        "plot_avg_run_times(avg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9CX83iqNNUP"
      },
      "source": [
        "# 7. Visualizing a run of ReloKmeans\n",
        "The code block below runs ReloKmeans on a desired data set and subsequently plots relevant information about the run and the clustering process. \n",
        "\n",
        "Time and scores are presented immediately below.\n",
        "\n",
        "The last code block presents a scatterplot of the data together with three centroid configurations. \n",
        "- Black X's represent centroid placements from initial clustering performed inside ReloKmeans\n",
        "- Magenta o's represent centroid placements after ReloKmeans has terminated, \n",
        "- Golden stars represent the ground truth."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkCrSLIhzXGo"
      },
      "source": [
        "#ENTER DESIRED VALUES BELOW\n",
        "\n",
        "random_initialization = True # True: uses Naïve Kmeans on initial clustering (ReloKmeans)\n",
        "                             # False: uses KMeans++ on initial clustering    (ReloKmeans++)\n",
        "\n",
        "n=1                          # Relocates n centroids at a time, default is 1.\n",
        "data_set_name = \"s1\"         # Which data set to perform clustering on\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "df, ground_truth_df, gt, k = read_data_and_gt(data_set_name)\n",
        "result_clustering, plotting_list, scores, run_times = ReloKmeans(df,random_initial=random_initialization, k=k, n=n)\n",
        "tot_time = time.time()-start\n",
        "\n",
        "#plt.plot([i+1 for i in range(len(run_times))], run_times) #Plots run times of KMeans within this run of ReloKmeans\n",
        "\n",
        "plt.plot([i+1 for i in range(len(scores))], scores) # Plots scores of each clustering result produced within this run of ReloKmeans\n",
        "\n",
        "print(\"Total time spent\".ljust(30), \": {0:.3f} seconds\".format(tot_time))\n",
        "print(\"First score achieved\".ljust(30), \": {}\".format(scores[0]))\n",
        "print(\"Lowest score achieved\".ljust(30), \": {}\".format(min(scores)))\n",
        "print(\"Score of returned solution\". ljust(30), \": {}\".format(score(result_clustering)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItCEDV7GpYor"
      },
      "source": [
        "# Code for visualizing the different centroid configurations on top of the data.\n",
        "\n",
        "def plot_centroids(plotting_list, gt_df=None):\n",
        "    \"\"\"\n",
        "    Function for plotting centroids and ground thruth (if given)\n",
        "    :param plotting_list: Sets of centroids from different runs to be plotted \n",
        "    :param gt_df: (Optional) Ground thruth centroids from data set source\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    num_centroids = len(plotting_list)\n",
        "\n",
        "    figure = plt.figure(figsize=[15,15])\n",
        "    \n",
        "    # Plot data points\n",
        "    plt.scatter(df['x'], df['y'], s=10)\n",
        "\n",
        "    # Plot ground truth if available\n",
        "    if gt_df is not None:\n",
        "        plt.scatter(gt_df[\"x\"], gt_df[\"y\"], marker=\"*\", c=\"gold\", s=600, linewidths=1, edgecolors=\"black\")\n",
        "\n",
        "    # Plot centroids in plotting list (normally only two sets of centroids)\n",
        "    for i, centroids in enumerate(plotting_list):\n",
        "        marker, color, size, alpha = (\"o\", \"magenta\", 50, 1) if i == num_centroids-1 else (\"X\", \"black\", 250, 1)\n",
        "\n",
        "        cent_x, cent_y = convert_to_pd_series(centroids)\n",
        "        plt.scatter(cent_x, cent_y, marker=marker, c=color, s=size, alpha=alpha)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKjVdTav3bq1"
      },
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = \"retina\"\n",
        "result = plotting_list[-1] # Last item represents centroids in returned solution (Centroids from termination clustering, i.e. when score increases, are not included in plotting list)\n",
        "initial = plotting_list[0] # First clustering performed by KMeans inside ReloKmeans\n",
        "\n",
        "print(\"Returned solution:\")\n",
        "print(\"Centroid Index:\", CI(gt, result))\n",
        "print()\n",
        "print(\"First solution:\")\n",
        "print(\"Centroid Index:\", CI(gt, plotting_list[0]))\n",
        "\n",
        "plot_centroids([initial, result], gt_df=ground_truth_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgyJyjiKPO-U"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}